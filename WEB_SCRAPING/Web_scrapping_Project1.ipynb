{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79f62cad",
   "metadata": {},
   "source": [
    "# WEB SCRAPING (Using Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348145a",
   "metadata": {},
   "source": [
    "# Project Objective : \n",
    "\n",
    "Website : https://quotes.toscrape.com/\n",
    "\n",
    "The objective of this project is to scrape quotes, author names, details, and tags from multiple pages of a website. \n",
    "\n",
    "    This involves:\n",
    "        Initializing a list to store data from multiple pages.\n",
    "        Looping through each page, sending a request to the website, and parsing the HTML content.\n",
    "        Extracting the desired information such as quote text, author's name, details link, and tags from each page.\n",
    "        Storing the extracted data in a list.\n",
    "        Converting the list of data into a DataFrame.\n",
    "        Printing the DataFrame and saving it to a CSV file for both single-page and multiple-page data.\n",
    "This code achieves the objective by utilizing libraries like requests for making HTTP requests, BeautifulSoup for parsing HTML, and pandas for handling data in DataFrame format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fb4db",
   "metadata": {},
   "source": [
    "# Pre-Requisite : \n",
    "\n",
    "    1.Basic knowledge of HTML and CSS is essential.\n",
    "    2.Proficiency in Python programming \n",
    "        o data types\n",
    "        o loops\t\n",
    "        o conditionals\n",
    "        o functions\n",
    "        o Web Scraping Libraries such as BeautifulSoup and Scrapy.\n",
    "    3.\tUnderstanding how to use these libraries to parse HTML and extract data is crucial.\n",
    "    4.\tUnderstand HTTP protocol and how to send HTTP requests using libraries like Requests in Python. Able to retrieve web pages and handle different types of responses.\n",
    "    5.\tCheck the website's robots.txt file and terms of service to ensure that web scraping is allowed.\n",
    "    6.\t Respect the website's terms and conditions to avoid legal issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291244db-bf5f-4726-9b99-5b057df16414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shubh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB 960.0 kB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.5/11.5 MB 5.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.5/11.5 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 16.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.5/11.5 MB 15.9 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.1/11.5 MB 15.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 15.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.3/11.5 MB 14.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.9/11.5 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.5/11.5 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.5 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.7/11.5 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.3/11.5 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.5 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.5/11.5 MB 13.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.1/11.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 13.9 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ab300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e688e",
   "metadata": {},
   "source": [
    "Code fetches the HTML content of a webpage using the requests library and then parses it using BeautifulSoup, making it ready for further processing and extraction of specific information from the webpage.\n",
    "\n",
    "    1.\tVariable link stores the URL of the website that we want to scrape. In this case, it's 'https://quotes.toscrape.com/'.\n",
    "    2.\trequests.get() function sends a GET request to the specified URL (link). This retrieves the HTML content of the webpage.\n",
    "    3.\tResponse from the website is stored in the variable res.\n",
    "    4.\tCreating a BeautifulSoup object:\n",
    "        a.\tBeautifulSoup is a Python library for parsing HTML and XML documents. \n",
    "        b.\tHere, BeautifulSoup(res.text, 'html.parser') creates BeautifulSoup object named soup to parse the HTML content of the response (res.text). \n",
    "        c.\tThis object is then used to navigate and extract data from the HTML structure of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492ae889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the website to scrape\n",
    "link = 'https://quotes.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e392a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a request to the website and getting the response\n",
    "res = requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e72256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a soup object to parse the HTML content\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a4823-e91b-4b22-9fbb-6c4eab7a6123",
   "metadata": {},
   "source": [
    "# Basic Initialization \n",
    "\n",
    "    1. Here we initialize two empty lists, quotes and authors, which will store the quotes and authors' names scraped from the website, respectively. \n",
    "    2. These lists are meant to accumulate the data extracted during the scraping process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ff3c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping quotes and authors' names\n",
    "\n",
    "# Initialize empty lists to store quotes and authors\n",
    "quotes = []\n",
    "authors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8107b722",
   "metadata": {},
   "source": [
    "# Scraping quotes and authors' names individually\n",
    "\n",
    "    •Program loops all elements of webpage with specified class 'text' (representing the quote text) using BeautifulSoup's find_all() method. \n",
    "    •\tFor each element found, text content of quote is extracted, removes the leading and trailing quotation marks [1:-1], and appends cleaned text to quotes list.\n",
    "    •\tSimilarly, it loops through all elements with the class 'author', representing the authors' names, extracts the text content (the authors' names), and appends them to the authors list.\n",
    "    •\tThis part of the code effectively scrapes the quotes and authors' names from the webpage and stores them in their respective lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fec3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all elements with the specified class and extract text\n",
    "for quote in soup.find_all('span', class_='text'):\n",
    "    quotes.append(quote.text[1:-1])  # Append the quote text\n",
    "\n",
    "# Loop through all elements with the specified class and extract text\n",
    "for i in soup.find_all('small', class_='author'):\n",
    "    authors.append(i.text)  # Append the author's name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddcc50-c68b-4f9e-9c0e-b5a7e7723e3c",
   "metadata": {},
   "source": [
    "# Scraping quotes, authors, details, and tags\n",
    "    For each element found on webpage with the class 'quote', it extracts specific information:\n",
    "    •\tThe text of the quote is extracted from the <span> element with the class 'text'.\n",
    "    •\tThe author's name is extracted from the <small> element with the class 'author'.\n",
    "    •\tThe details link is extracted from the <a> element within the quote.\n",
    "    •\tTags associated with the quote are extracted from <a> elements with the class 'tag'.\n",
    "    •\tAfter extracting above information, \n",
    "        o\tWe need to formats the tag into a comma-separated string \n",
    "        o\tappends all the extracted data (quote text, author's name, details link, and tags) as a list to the data list. \n",
    "    •\tThis part effectively collects the required information for each quote and stores it in a structured format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b19c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data list to store the scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through all elements with the specified class and extract desired information\n",
    "for sp in soup.find_all('div', class_='quote'):\n",
    "    quote = sp.find('span', class_='text').text[1:-1]  # Extract quote text\n",
    "    author = sp.find('small', class_='author').text  # Extract author's name\n",
    "    details = sp.find('a').get('href')  # Extract details link\n",
    "    tags = [tag.text for tag in sp.find_all('a', class_='tag')]  # Extract tags\n",
    "    tags = ','.join(tags)  # Convert tags list to a comma-separated string\n",
    "    data.append([quote, author, details, tags])  # Append data to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c993a2e-261b-4e33-ac7d-d500b1acab04",
   "metadata": {},
   "source": [
    "# Scraping data from multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f98a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store data from multiple pages\n",
    "multiple_pages_data = []\n",
    "\n",
    "# Loop through multiple pages and scrape data\n",
    "for page in range(1, 11):\n",
    "    # Construct the URL for each page\n",
    "    page_link = f\"http://quotes.toscrape.com/page/{page}\"\n",
    "    # Send a request to the website\n",
    "    res = requests.get(page_link)\n",
    "    # Create a soup object to parse the HTML content\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    # Loop through all elements with the specified class and extract desired information\n",
    "    for sp in soup.find_all('div', class_='quote'):\n",
    "        quote = sp.find('span', class_='text').text[1:-1]  # Extract quote text\n",
    "        author = sp.find('small', class_='author').text  # Extract author's name\n",
    "        details = sp.find('a').get('href')  # Extract details link\n",
    "        tags = [tag.text for tag in sp.find_all('a', class_='tag')]  # Extract tags\n",
    "        tags = ','.join(tags)  # Convert tags list to a comma-separated string\n",
    "        multiple_pages_data.append([quote, author, details, tags])  # Append data to the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea34da-e4bd-4e18-8dd2-b08d7a87436a",
   "metadata": {},
   "source": [
    "# Data processing and saving\n",
    "\n",
    "    •Convert Data to DataFrame (Single Page) with Column names 'Quote', 'Author', 'Details', and 'Tags'.\n",
    "    •Print DataFrame (Single Page) to the console\n",
    "    •Save DataFrame to CSV (Single Page):\n",
    "    •The DataFrame is saved to a CSV file named 'scraped_data.csv' using the to_csv() method provided by pandas.\n",
    "    •The parameter index=False ensures that the DataFrame index is not included in the CSV file.\n",
    "    •Similarly, all above steps for Multiple Pages\n",
    "    •Finally, a confirmation message is printed to console, indicating that scraped data from multiple pages has been successfully saved to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64a0afaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped Data from Single Page:\n",
      "                                               Quote             Author  \\\n",
      "0  The world as we have created it is a process o...    Albert Einstein   \n",
      "1  It is our choices, Harry, that show what we tr...       J.K. Rowling   \n",
      "2  There are only two ways to live your life. One...    Albert Einstein   \n",
      "3  The person, be it gentleman or lady, who has n...        Jane Austen   \n",
      "4  Imperfection is beauty, madness is genius and ...     Marilyn Monroe   \n",
      "5  Try not to become a man of success. Rather bec...    Albert Einstein   \n",
      "6  It is better to be hated for what you are than...         André Gide   \n",
      "7  I have not failed. I've just found 10,000 ways...   Thomas A. Edison   \n",
      "8  A woman is like a tea bag; you never know how ...  Eleanor Roosevelt   \n",
      "9   A day without sunshine is like, you know, night.       Steve Martin   \n",
      "\n",
      "                     Details                                      Tags  \n",
      "0    /author/Albert-Einstein       change,deep-thoughts,thinking,world  \n",
      "1        /author/J-K-Rowling                         abilities,choices  \n",
      "2    /author/Albert-Einstein  inspirational,life,live,miracle,miracles  \n",
      "3        /author/Jane-Austen             aliteracy,books,classic,humor  \n",
      "4     /author/Marilyn-Monroe                 be-yourself,inspirational  \n",
      "5    /author/Albert-Einstein                   adulthood,success,value  \n",
      "6         /author/Andre-Gide                                 life,love  \n",
      "7    /author/Thomas-A-Edison  edison,failure,inspirational,paraphrased  \n",
      "8  /author/Eleanor-Roosevelt           misattributed-eleanor-roosevelt  \n",
      "9       /author/Steve-Martin                      humor,obvious,simile  \n",
      "Scraped data saved to 'scraped_data.csv'.\n",
      "\n",
      "Scraped Data from Multiple Pages:\n",
      "                                                Quote              Author  \\\n",
      "0   The world as we have created it is a process o...     Albert Einstein   \n",
      "1   It is our choices, Harry, that show what we tr...        J.K. Rowling   \n",
      "2   There are only two ways to live your life. One...     Albert Einstein   \n",
      "3   The person, be it gentleman or lady, who has n...         Jane Austen   \n",
      "4   Imperfection is beauty, madness is genius and ...      Marilyn Monroe   \n",
      "..                                                ...                 ...   \n",
      "95  You never really understand a person until you...          Harper Lee   \n",
      "96  You have to write the book that wants to be wr...   Madeleine L'Engle   \n",
      "97  Never tell the truth to people who are not wor...          Mark Twain   \n",
      "98          A person's a person, no matter how small.           Dr. Seuss   \n",
      "99  ... a mind needs books as a sword needs a whet...  George R.R. Martin   \n",
      "\n",
      "                      Details  \\\n",
      "0     /author/Albert-Einstein   \n",
      "1         /author/J-K-Rowling   \n",
      "2     /author/Albert-Einstein   \n",
      "3         /author/Jane-Austen   \n",
      "4      /author/Marilyn-Monroe   \n",
      "..                        ...   \n",
      "95         /author/Harper-Lee   \n",
      "96   /author/Madeleine-LEngle   \n",
      "97         /author/Mark-Twain   \n",
      "98           /author/Dr-Seuss   \n",
      "99  /author/George-R-R-Martin   \n",
      "\n",
      "                                                 Tags  \n",
      "0                 change,deep-thoughts,thinking,world  \n",
      "1                                   abilities,choices  \n",
      "2            inspirational,life,live,miracle,miracles  \n",
      "3                       aliteracy,books,classic,humor  \n",
      "4                           be-yourself,inspirational  \n",
      "..                                                ...  \n",
      "95                                better-life-empathy  \n",
      "96  books,children,difficult,grown-ups,write,write...  \n",
      "97                                              truth  \n",
      "98                                      inspirational  \n",
      "99                                         books,mind  \n",
      "\n",
      "[100 rows x 4 columns]\n",
      "Scraped data from multiple pages saved to 'scraped_data_multiple_pages.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Quote', 'Author', 'Details', 'Tags'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Scraped Data from Single Page:\")\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_data.csv', index=False)\n",
    "print(\"Scraped data saved to 'scraped_data.csv'.\")\n",
    "\n",
    "# Convert the list of data from multiple pages into a DataFrame\n",
    "df_multiple_pages = pd.DataFrame(multiple_pages_data, columns=['Quote', 'Author', 'Details', 'Tags'])\n",
    "\n",
    "# Print the DataFrame from multiple pages\n",
    "print(\"\\nScraped Data from Multiple Pages:\")\n",
    "print(df_multiple_pages)\n",
    "\n",
    "# Save the DataFrame from multiple pages to a CSV file\n",
    "df_multiple_pages.to_csv('scraped_data_multiple_pages.csv', index=False)\n",
    "print(\"Scraped data from multiple pages saved to 'scraped_data_multiple_pages.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2941e-ca7d-4029-8bc0-bc0af35a5fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
